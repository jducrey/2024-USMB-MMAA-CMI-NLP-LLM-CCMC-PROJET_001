# PROJET_001
Projet en Traitement du Langage Naturel (NLP)
Exploration des concepts et des mécanismes, à la base du fonctionnement des Grands Modèles de Langage (LLM) et des Chaînes de Markov Conditionnée au Contexte (CCMC),
au travers de recherches personnelles et de la lecture d'un article, traitant de ce sujet.

Parmi les notions abordées, on compte notamment: 
- Tokenizer (SpaCy, BPE, Wordpiece)
- Word Embedding (TF-IDF, Word2Vec)
- Self-Attention
- Transformer Architecture
- Context-Conditionned Markov Chain (Transition Matrix, Prompt Mask)

### Bijection between LLM and CCMC, on their probabilities distributions.
### Prediction based on the structure of a vector space derived from word embeddings.

#### Article étudié:
From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers.
M. Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, Samet Oymak.  
Lien de l'article utilisé pour l'étude: https://arxiv.org/abs/2402.13512
