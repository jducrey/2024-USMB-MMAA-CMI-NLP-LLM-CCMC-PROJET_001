# 2024-USMB-MMAA-CMI-NLP - LLM & CCMC : Une Exploration Contextuelle

Projet universitaire en **Traitement Automatique du Langage Naturel (NLP)**, combinant recherche autodidacte et analyse dâ€™un article de recherche de pointe.  

L'objectif : Comprendre et illustrer les concepts et les mÃ©canismes internes, Ã  la base du fonctionnement des **Grands ModÃ¨les de Langage (LLM)** et des **ChaÃ®nes de Markov ConditionnÃ©es au Contexte (CCMC)**.

---

## ğŸ¯ Objectifs

- Explorer les concepts fondamentaux des LLM, Ã  travers une approche thÃ©orique et appliquÃ©e.
- Mettre en lumiÃ¨re les **liens entre les architectures Transformers et les chaÃ®nes de Markov contextuelles**.
- Analyser comment la structure vectorielle des reprÃ©sentations lexicales influence la prÃ©diction.

---

## ğŸ§© Notions abordÃ©es

- **Tokenization** : SpaCy, Byte Pair Encoding (BPE), WordPiece
- **Word Embeddings** : TF-IDF, Word2Vec
- **Self-Attention** & **Architecture Transformer**
- **ChaÃ®ne de Markov ConditionnÃ©es au Contexte ou Context-Conditionned Markov Chain (CCMC)**
  - Matrice de transition
  - Masque de Prompt (Prompt Mask)
- **Distribution de probabilitÃ© dans les LLM vs CCMC**

---

## ğŸ”¬ Analyse thÃ©orique

### ğŸ”„ Bijection entre LLM et CCMC
> Ã‰tude des correspondances entre les distributions de probabilitÃ© gÃ©nÃ©rÃ©es par un LLM et celles issues dâ€™une chaÃ®ne de Markov contextuelle.

### ğŸ§­ PrÃ©diction dans lâ€™espace vectoriel
> Analyse de la dynamique de prÃ©diction dans un espace dÃ©rivÃ© de lâ€™embedding des mots.

---

## ğŸ“š Article de rÃ©fÃ©rence
**"From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers"**.
_M. Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, Samet Oymak_  
ğŸ”— [Lire lâ€™article sur arXiv](https://arxiv.org/abs/2402.13512)

---

## ğŸ“ Contexte acadÃ©mique

Projet rÃ©alisÃ© dans le cadre du **Master MMAA â€“ UniversitÃ© Savoie Mont Blanc**  
Approche mÃªlant rigueur mathÃ©matique et curiositÃ© personnelle sur les enjeux modernes de lâ€™IA.

---

## ğŸ’¬ Suggestions, remarques, questions ?
N'hÃ©sitez pas Ã  ouvrir une *issue* ou me contacter directement via GitHub !
